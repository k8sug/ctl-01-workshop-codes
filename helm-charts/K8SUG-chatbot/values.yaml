# Default values for k8sug-chatbot.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
backend:
  replicaCount: 1
  image:
    repository: docker.io/thiago4go/godel-1_1-api
    tag: 1.0.2
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 5000

frontend:
  replicaCount: 1
  image:
    repository: docker.io/thiago4go/chatbot-fronted-flask
    tag: 0.0.2
    pullPolicy: IfNotPresent
  service:
    targetPort : 8000
    port: 80

config:
  INSTRUCTION: "Instruction: Given a dialogue context, respond in a fun and empathic manner as 'k8sug-ctl'. Engage the user with interesting facts, humorous anecdotes, and simple explanations about LLMs and Kubernetes. Ensure responses are friendly and supportive, making technical concepts approachable and enjoyable to learn about, all while maintaining your identity as 'k8sug-ctl'."
  KNOWLEDGE: "knowledge: Large Language Models (LLMs) like GPT are advanced forms of artificial intelligence that generate human-like text by learning from a vast dataset of diverse textual content. A LLM can help draft an academic essay or a blog post by providing a structured narrative based on the input it receives. Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Kubernetes can manage a multi-container application running across several machines, handling tasks such as traffic management, scaling according to demand, and monitoring. Common use cases include cloud-native development, microservices management, and hybrid cloud applications.Additionally, Kubernetes simplifies deployment processes such as rolling updates and can revert to previous versions without downtime, ensuring high availability of applications. Some of the key features of Kubernetes include automatic scaling, self-healing, and load balancing. user:How does Kubernetes handle updates and rollbacks? k8sug-ctl:Kubernetes supports rolling updates, which allow you to deploy new versions of your application without downtime. It gradually replaces the old version with the new one, ensuring a smooth transition and minimal disruption to users. If an update fails, Kubernetes can automatically roll back to the previous version, maintaining the availability of your application. user:That's great to know! Thanks for the explanation. k8sug-ctl:You're welcome! I'm here to help with any questions you have about Kubernetes or Large Language Models. Feel free to ask me anything. K8SUG - the most active Kubernetes + AI meetup! We are enabling anyone interested in Kubernetes and AI from anywhere to join online or in person in Singapore, Australia, Canada, UK and more to come. We meet to discuss anything Kubernetes / OpenShift related including but not limited to how to Build, Operate, and Manager Kubernetes Clusters, how to Secure and Backup containers, Migrate containers between On-Premises and across Multi-Cloud, how the DR works for the containers, etc. Anyone using or planning to adopt Kubernetes should join us to learn or share their experiences on Kubernetes. It can be vanilla Kubernetes or any managed Kubernetes or OpenShift either OnPrem or in the Public or Private Cloud. What Kubernetes is? Kubernetes effectively acts as a 'Cloud Operating System' for distributed systems or applications running on a cluster of physical or virtual machines, specifically designed to manage large groups of containerized applications. What Kubernetes does? Abstraction: Kubernetes creates a unified interface for applications, shielding them from the underlying infrastructure complexities. Whether apps run on physical servers on primise, virtual machines in the cloud, or a mix of both, Kubernetes provides a consistent environment. This makes it easier to move and scale apps. API-Driven Interaction: Everything in Kubernetes is controlled through its API. This makes it easy to integrate with your existing IT tools and workflows. You can use command-line tools, graphical interfaces, or even write scripts to automate tasks and interact with a Kubernetes environment. Orchestration: Kubernetes automates the entire lifecycle of applications â€“ from deployment and scaling to updates and self-healing. It monitors the health of applications and infrastructure, restarting or replacing components as needed to keep everything running smoothly. Resource Management: Kubernetes acts like a traffic controller, dynamically allocating resources like CPU, memory, and storage to applications. It optimizes the utilization of infrastructure, ensuring  apps get the resources they need while preventing resource waste. Main Components of a Kubernetes Cluster Control Plane: The control plane manages the state of the Kubernetes cluster, including scheduling and responding to cluster events. API Server: Acts as the frontend to the cluster, exposing the Kubernetes API and serving as the gateway for all internal and external communications. etcd: A lightweight, distributed key-value store that stores all cluster data, ensuring there is a single source of truth for the state of the cluster. Scheduler: Responsible for assigning work, such as Pods, to Nodes based on resource availability and other constraints. Controller Manager: Runs controller processes to regulate the state of the cluster, managing routine tasks such as replicating components and handling node operations. Kubelet: An agent running on each node, ensuring that containers are running in a Pod and healthy as per the specifications. Kube Proxy: Maintains network rules on nodes, allowing network communication to Pods from network sessions inside or outside of the cluster. Container Runtime: The software responsible for running containers, with Docker being the most known example, though Kubernetes supports several alternatives. Fundamentals Concepts of Kubernetes Pods: The smallest deployable units in Kubernetes, each Pod represents a single instance of a running process in your cluster. Nodes: These are the worker machines within a Kubernetes cluster, which can be either physical or virtual systems. Clusters: Comprises a group of nodes that work together to run containerized applications. Kubernetes manages these applications across the cluster. Services: A Service in Kubernetes defines a logical set of Pods and a policy by which to access them, often through a stable IP address. Deployments: Manage the desired state of Pods declaratively, facilitating updates and scaling for applications. Namespaces: Virtual clusters within the same physical cluster, Namespaces are used to divide cluster resources between multiple users. ConfigMaps and Secrets: ConfigMaps handle non-confidential configuration data, whereas Secrets manage sensitive information. Both separate configuration from application code. Persistent Volumes: Provides an abstraction layer for storage in the cluster, decoupling the specifics of storage provisioning from its consumption. ReplicaSets: Ensures that a specified number of Pod replicas are running at any given time for redundancy and scaling. What is LLM? Large Language Models (LLMs) like GPT have ushered in a new era of artificial intelligence (AI). These models, trained on massive datasets, can generate remarkably human-like text, translate languages, create content, and answer complex questions. Their potential applications span industries, from customer service and content creation to healthcare and education. Why Kubernetes for LLM Deployment? The computational demands of LLMs are immense. Kubernetes, the industry-standard container orchestration platform, is uniquely suited to address these challenges: Scalability: Kubernetes allows you to easily scale LLM deployments up or down to meet fluctuating demand. Resource Efficiency: Kubernetes optimizes resource allocation (CPUs, GPUs, memory) to ensure efficient utilisation. Fault Tolerance: Kubernetes automatically restarts failed containers, ensuring high availability. Flexibility: Kubernetes works seamlessly with cloud providers and on-premise infrastructure. Key Considerations for LLM Deployment Model Size: LLMs can be massive. Choose a model size that balances performance and resource requirements. Hardware Acceleration: GPUs (or specialised AI accelerators) are essential for accelerating inference and training. Serving Infrastructure: Choose a serving framework that optimises LLM performance. Monitoring and Optimisation: Continuously monitor and fine-tune your LLM deployment for optimal performance and cost-effectiveness"
  MAX_LENGTH: "128"
  MIN_LENGTH: "8"
  TOP_P: "0.9"
  DO_SAMPLE: "True"

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
